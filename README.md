# Synesthesia-AIArtathon
Final Project submission for the AI Artathon

# Project Brief Description:
Synesthesia is a neurological condition in which information meant to stimulate one of the senses activates several of them instead. Our project is a Virtual Reality Show that visualizes music to others, using AI generated images and music. We combine our original melodies with AI generated harmony to create an orchestra, then use it to generate visuals. We want people to see and interact with the good vibrations from our music,  and allow people with hearing loss to perceive it using other senses.

# Project Art Description:
Imagine making an original piece, turning it into an orchestra with different styles, then getting the audience to interact with its amazing vibes in a virtual reality world, all with the help of AI! 
Hadieh wrote an original piano score and used Apollo to turn it into a happy game theme. In the Celine Dion song, all the harmonies and rhythms involved were AI generated based on the piano/flute melody Hadieh made.
For one of the song’s videos an implementation of 3D Ken Burns Effect from a Single Image (painting in our case) using PyTorch. In addition, we also used SinGAN for Generating Images and Animations in Google Colab.
Music Midi tracks are used as input into Apollo - a corpus-based machine learning AI interface - to generate melodies, harmonies, rhythms,  and orchestrations using the original input as a model. After that, the audio output is processed through an AI based BigGAN music visualizer.. Finally, we process the videos inside of Unity’s HDPR (High Definition Pipeline Rendering) which translate the video into a particle -based system to create an immersive VR enviroment which can be viewed  as a 360 video.

# Additional Art Material
Some of the original and AI generated music along with some of the videos of the raw project without VR can be found in the folder ProjectOutputs.

# Project Technologies Description:
The technology stack for this project is composed of the following:
* Apollo: an interactive environment using corpus-based style imitation with models trained on a set of different instruments for symbolic music generation.
* Deep visualizer: a music visualizer that takes the output file of the musical piece generated by Apollo and produces a visualization using BigGan based on pitch, tempo, and a few other parameters.
* Unity: the video is processed by unity to produce a world constructed of a particle system whose properties are influenced by the input video. 
* Unity’s visual effect graph was used to program the particle system.

# Model References:
* Apollo: https://apollo.iat.sfu.ca/
* Deep visualizer: https://github.com/msieg/deep-music-visualizer
* an implementation of 3D Ken Burns Effect from a Single Image using PyTorch: https://pythonrepo.com/repo/sniklaus-3d-ken-burns-python-deep-learning
* SineGan: https://github.com/dvschultz/ai/blob/master/SinGAN.ipynb

# Note:
Original videos for bassy time traveler were not included in the repo because it exceeds the GitHub allowed file size and therefore might affect the import on the scene.
If needed I can send the video upon request.





